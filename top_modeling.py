# -*- coding: utf-8 -*-
"""Top modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iM8LZj9sx-imNMCHY1g2b0cy_52LO0Mz
"""

# Install necessary libraries
!pip install nltk spacy gensim pyldavis scikit-learn matplotlib fasttext

import nltk
import spacy
import gensim
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from gensim import corpora, models
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.datasets import fetch_20newsgroups
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Download necessary nltk resources
nltk.download('punkt')
nltk.download('stopwords')

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Load dataset (using a smaller subset to speed up processing)
data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))
documents = data.data[:1000]  # Use a smaller subset for faster processing

# Text preprocessing
def preprocess_text(text):
    doc = nlp(text.lower())
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    return tokens

preprocessed_docs = [preprocess_text(doc) for doc in documents]

# Create dictionary and corpus for LDA
texts = [doc for doc in preprocessed_docs]  # Directly use tokens for LDA
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Topic modeling with LDA
lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=5)  # Reduce passes for speed
topics = lda_model.print_topics(num_words=5)

print("LDA Topics:")
for topic in topics:
    print(topic)

# Visualize LDA topics with pyLDAvis
vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(vis)

# Generate document vectors using LDA topic distributions
lda_topic_distributions = [lda_model.get_document_topics(bow) for bow in corpus]

# Ensure all document vectors have the same length
num_topics = lda_model.num_topics
lda_vectors = np.zeros((len(lda_topic_distributions), num_topics))

for i, doc_topics in enumerate(lda_topic_distributions):
    for topic_id, topic_prob in doc_topics:
        lda_vectors[i, topic_id] = topic_prob

# Calculate cosine similarity
similarity_matrix = cosine_similarity(lda_vectors)

# Cluster the documents (example using KMeans clustering)
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, n_init=5, max_iter=100)  # Reduced n_init for speed
kmeans.fit(lda_vectors)
clusters = kmeans.labels_

# Add clusters to DataFrame for analysis
df = pd.DataFrame({
    'document': documents,
    'cluster': clusters
})

# Display the first few rows of the DataFrame
print(df.head())

# Visualize clustering results
plt.figure(figsize=(10, 8))
plt.scatter(range(len(clusters)), clusters, c=clusters, cmap='viridis')
plt.xlabel('Document Index')
plt.ylabel('Cluster')
plt.title('Document Clustering')
plt.colorbar()
plt.show()

